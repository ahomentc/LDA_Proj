{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data\n",
    "data = pd.read_csv('wiki_movie_plots_deduped.csv')\n",
    "data = np.array(data)\n",
    "plots = list(data[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/andrei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing data\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = list(map(preprocess, plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bartender is working at a saloon, serving drinks to customers. After he fills a stereotypically Irish man's bucket with beer, Carrie Nation and her followers burst inside. They assault the Irish man, pulling his hat over his eyes and then dumping the beer over his head. The group then begin wrecking the bar, smashing the fixtures, mirrors, and breaking the cash register. The bartender then sprays seltzer water in Nation's face before a group of policemen appear and order everybody to leave.[1]\n",
      "['bartend', 'work', 'saloon', 'serv', 'drink', 'custom', 'fill', 'stereotyp', 'irish', 'bucket', 'beer', 'carri', 'nation', 'follow', 'burst', 'insid', 'assault', 'irish', 'pull', 'eye', 'dump', 'beer', 'head', 'group', 'begin', 'wreck', 'smash', 'fixtur', 'mirror', 'break', 'cash', 'regist', 'bartend', 'spray', 'seltzer', 'water', 'nation', 'face', 'group', 'policemen', 'appear', 'order', 'everybodi', 'leav']\n"
     ]
    }
   ],
   "source": [
    "print(plots[0])\n",
    "print(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of the words\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "'''\n",
    "Filter out tokens that appear in less than 15 documents (absolute number) or\n",
    "more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary reporting how many words and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tfidf\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/andrei/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 334, in worker_e_step\n",
      "    chunk_no, chunk, worker_lda = input_queue.get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/andrei/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 334, in worker_e_step\n",
      "    chunk_no, chunk, worker_lda = input_queue.get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/andrei/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 334, in worker_e_step\n",
      "    chunk_no, chunk, worker_lda = input_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-d29fcdbb305e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training our lda model using bag of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;31m# wait for all outstanding jobs to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreallen\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[0;34m(force)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mupdateafter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlencorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training LDA model using %i processes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mlog_perplexity\u001b[0;34m(self, chunk, total_docs)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0msubsample_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mperwordbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubsample_ratio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubsample_ratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         logger.info(\n\u001b[1;32m    823\u001b[0m             \u001b[0;34m\"%.3f per-word bound, %.1f perplexity estimate based on a held-out corpus of %i documents with %i words\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mbound\u001b[0;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[1;32m   1085\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bound: at document #%i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m                 \u001b[0mgammad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/classes/179/env/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;31m# Initialize the variational distribution q(theta|gamma) for the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         \u001b[0mElogtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m         \u001b[0mexpElogtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training our lda model using bag of words\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=30, id2word=dictionary, passes=5, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"kill\" + 0.007*\"krishna\" + 0.006*\"tell\" + 0.006*\"famili\" + 0.005*\"leav\" + 0.005*\"devi\" + 0.005*\"return\" + 0.005*\"friend\" + 0.005*\"john\" + 0.005*\"go\"\n",
      "Topic: 1 \n",
      "Words: 0.012*\"kill\" + 0.006*\"attack\" + 0.005*\"escap\" + 0.004*\"forc\" + 0.004*\"destroy\" + 0.004*\"leav\" + 0.004*\"reveal\" + 0.004*\"ship\" + 0.004*\"earth\" + 0.004*\"return\"\n",
      "Topic: 2 \n",
      "Words: 0.009*\"tell\" + 0.009*\"leav\" + 0.006*\"go\" + 0.006*\"friend\" + 0.005*\"time\" + 0.005*\"life\" + 0.005*\"home\" + 0.005*\"come\" + 0.005*\"ask\" + 0.005*\"father\"\n",
      "Topic: 3 \n",
      "Words: 0.007*\"murder\" + 0.006*\"kill\" + 0.006*\"leav\" + 0.005*\"mari\" + 0.005*\"jimmi\" + 0.004*\"tell\" + 0.004*\"discov\" + 0.004*\"hous\" + 0.004*\"paul\" + 0.004*\"race\"\n",
      "Topic: 4 \n",
      "Words: 0.007*\"kill\" + 0.005*\"tell\" + 0.005*\"leav\" + 0.005*\"david\" + 0.004*\"polic\" + 0.004*\"take\" + 0.004*\"terrorist\" + 0.004*\"meet\" + 0.004*\"jam\" + 0.004*\"go\"\n",
      "Topic: 5 \n",
      "Words: 0.014*\"love\" + 0.011*\"father\" + 0.010*\"marri\" + 0.009*\"famili\" + 0.008*\"friend\" + 0.008*\"come\" + 0.007*\"mother\" + 0.006*\"get\" + 0.006*\"girl\" + 0.006*\"meet\"\n",
      "Topic: 6 \n",
      "Words: 0.007*\"radha\" + 0.006*\"leav\" + 0.005*\"love\" + 0.005*\"tell\" + 0.005*\"film\" + 0.005*\"harri\" + 0.004*\"take\" + 0.004*\"johnni\" + 0.004*\"marri\" + 0.004*\"wife\"\n",
      "Topic: 7 \n",
      "Words: 0.006*\"leav\" + 0.006*\"tell\" + 0.005*\"jack\" + 0.005*\"hous\" + 0.004*\"polic\" + 0.004*\"kill\" + 0.004*\"take\" + 0.004*\"go\" + 0.004*\"home\" + 0.004*\"return\"\n",
      "Topic: 8 \n",
      "Words: 0.010*\"kill\" + 0.007*\"king\" + 0.005*\"leav\" + 0.005*\"fight\" + 0.005*\"take\" + 0.005*\"power\" + 0.004*\"return\" + 0.004*\"tell\" + 0.004*\"help\" + 0.004*\"reveal\"\n",
      "Topic: 9 \n",
      "Words: 0.015*\"kill\" + 0.015*\"polic\" + 0.006*\"money\" + 0.006*\"gang\" + 0.006*\"offic\" + 0.006*\"take\" + 0.005*\"murder\" + 0.005*\"escap\" + 0.004*\"shoot\" + 0.004*\"plan\"\n"
     ]
    }
   ],
   "source": [
    "# explore the words occuring in each topic and their relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using TF-IDF\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=30, id2word=dictionary, passes=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.004*\"jerri\" + 0.003*\"charli\" + 0.003*\"vampir\" + 0.002*\"team\" + 0.002*\"jess\" + 0.002*\"clan\" + 0.002*\"tournament\" + 0.002*\"nick\" + 0.002*\"game\" + 0.002*\"kill\"\n",
      "\n",
      "Topic: 1 Word: 0.041*\"yuki\" + 0.012*\"bella\" + 0.011*\"backdrop\" + 0.011*\"gemini\" + 0.011*\"sunder\" + 0.010*\"cosmonaut\" + 0.009*\"calvin\" + 0.009*\"choir\" + 0.008*\"goro\" + 0.007*\"yuan\"\n",
      "\n",
      "Topic: 2 Word: 0.043*\"viru\" + 0.013*\"indra\" + 0.012*\"stan\" + 0.011*\"heidi\" + 0.009*\"ginni\" + 0.008*\"loop\" + 0.008*\"aborigin\" + 0.008*\"bianca\" + 0.008*\"cheng\" + 0.007*\"olli\"\n",
      "\n",
      "Topic: 3 Word: 0.030*\"sakura\" + 0.017*\"kevin\" + 0.012*\"princess\" + 0.011*\"mask\" + 0.010*\"charley\" + 0.009*\"special\" + 0.009*\"wade\" + 0.009*\"omar\" + 0.008*\"aslan\" + 0.007*\"gang\"\n",
      "\n",
      "Topic: 4 Word: 0.118*\"raja\" + 0.047*\"dong\" + 0.018*\"kabir\" + 0.009*\"hana\" + 0.008*\"lena\" + 0.007*\"partner\" + 0.007*\"cheryl\" + 0.006*\"dinosaur\" + 0.006*\"romeo\" + 0.006*\"volleybal\"\n",
      "\n",
      "Topic: 5 Word: 0.049*\"rama\" + 0.033*\"vishnu\" + 0.030*\"ganga\" + 0.017*\"shaman\" + 0.016*\"gandhi\" + 0.014*\"elementari\" + 0.011*\"tobi\" + 0.009*\"kala\" + 0.009*\"suspens\" + 0.007*\"samson\"\n",
      "\n",
      "Topic: 6 Word: 0.042*\"terrorist\" + 0.018*\"guru\" + 0.010*\"kung\" + 0.009*\"venu\" + 0.008*\"sato\" + 0.008*\"thriller\" + 0.008*\"supernatur\" + 0.007*\"joshua\" + 0.007*\"gian\" + 0.007*\"kashi\"\n",
      "\n",
      "Topic: 7 Word: 0.046*\"yakuza\" + 0.025*\"chandra\" + 0.017*\"nana\" + 0.013*\"istanbul\" + 0.011*\"saga\" + 0.010*\"bangkok\" + 0.009*\"lala\" + 0.007*\"cassandra\" + 0.006*\"comet\" + 0.005*\"greta\"\n",
      "\n",
      "Topic: 8 Word: 0.030*\"sook\" + 0.027*\"sunni\" + 0.023*\"kira\" + 0.022*\"cyborg\" + 0.011*\"cosmo\" + 0.011*\"mitch\" + 0.010*\"antoni\" + 0.008*\"maharaja\" + 0.008*\"doppelgang\" + 0.007*\"portal\"\n",
      "\n",
      "Topic: 9 Word: 0.034*\"singh\" + 0.026*\"rana\" + 0.012*\"woodi\" + 0.010*\"ottoman\" + 0.010*\"yamamoto\" + 0.009*\"sofia\" + 0.009*\"zhao\" + 0.009*\"rina\" + 0.008*\"rooster\" + 0.007*\"pasha\"\n",
      "\n",
      "Topic: 10 Word: 0.021*\"rowdi\" + 0.013*\"melodrama\" + 0.012*\"masha\" + 0.011*\"nate\" + 0.011*\"superhero\" + 0.011*\"christina\" + 0.010*\"devan\" + 0.009*\"mall\" + 0.009*\"ping\" + 0.009*\"ting\"\n",
      "\n",
      "Topic: 11 Word: 0.049*\"hari\" + 0.011*\"arti\" + 0.009*\"superman\" + 0.008*\"shane\" + 0.008*\"walli\" + 0.008*\"rugbi\" + 0.007*\"katrina\" + 0.007*\"kenni\" + 0.007*\"remi\" + 0.006*\"dreyfu\"\n",
      "\n",
      "Topic: 12 Word: 0.003*\"love\" + 0.003*\"father\" + 0.003*\"famili\" + 0.003*\"marri\" + 0.003*\"mother\" + 0.002*\"girl\" + 0.002*\"friend\" + 0.002*\"school\" + 0.002*\"wife\" + 0.002*\"life\"\n",
      "\n",
      "Topic: 13 Word: 0.003*\"villag\" + 0.003*\"kill\" + 0.003*\"film\" + 0.002*\"stori\" + 0.002*\"famili\" + 0.002*\"father\" + 0.002*\"polic\" + 0.002*\"love\" + 0.002*\"brother\" + 0.002*\"life\"\n",
      "\n",
      "Topic: 14 Word: 0.008*\"earth\" + 0.008*\"alien\" + 0.008*\"monster\" + 0.007*\"ship\" + 0.005*\"planet\" + 0.005*\"island\" + 0.005*\"human\" + 0.004*\"space\" + 0.004*\"destroy\" + 0.004*\"power\"\n",
      "\n",
      "Topic: 15 Word: 0.020*\"bong\" + 0.018*\"kato\" + 0.016*\"sweeti\" + 0.015*\"sophi\" + 0.012*\"reaper\" + 0.011*\"milo\" + 0.011*\"nagasaki\" + 0.009*\"feudal\" + 0.006*\"surfer\" + 0.006*\"afghanistan\"\n",
      "\n",
      "Topic: 16 Word: 0.113*\"jung\" + 0.047*\"jong\" + 0.012*\"korean\" + 0.010*\"mick\" + 0.008*\"joanna\" + 0.008*\"martian\" + 0.007*\"theo\" + 0.006*\"nell\" + 0.006*\"greg\" + 0.005*\"vendor\"\n",
      "\n",
      "Topic: 17 Word: 0.031*\"devi\" + 0.012*\"dynasti\" + 0.010*\"daisi\" + 0.009*\"jill\" + 0.008*\"akbar\" + 0.006*\"violet\" + 0.006*\"ella\" + 0.005*\"housemaid\" + 0.005*\"amber\" + 0.005*\"daffi\"\n",
      "\n",
      "Topic: 18 Word: 0.082*\"radha\" + 0.026*\"bombay\" + 0.020*\"sasha\" + 0.019*\"yamato\" + 0.017*\"shogun\" + 0.015*\"chung\" + 0.015*\"ming\" + 0.012*\"rick\" + 0.011*\"maxim\" + 0.011*\"shaolin\"\n",
      "\n",
      "Topic: 19 Word: 0.048*\"shyam\" + 0.023*\"lupin\" + 0.020*\"ranjit\" + 0.010*\"nath\" + 0.009*\"transgend\" + 0.008*\"ritchi\" + 0.006*\"kathi\" + 0.006*\"diva\" + 0.005*\"mothership\" + 0.005*\"bain\"\n",
      "\n",
      "Topic: 20 Word: 0.035*\"conan\" + 0.011*\"osaka\" + 0.010*\"brahmin\" + 0.009*\"chow\" + 0.008*\"travi\" + 0.008*\"hitman\" + 0.007*\"gong\" + 0.007*\"teddi\" + 0.007*\"calcutta\" + 0.007*\"matti\"\n",
      "\n",
      "Topic: 21 Word: 0.060*\"ramu\" + 0.058*\"maya\" + 0.028*\"baba\" + 0.017*\"miki\" + 0.012*\"ahm\" + 0.010*\"cobra\" + 0.008*\"mako\" + 0.007*\"firefli\" + 0.006*\"zeu\" + 0.006*\"tatiana\"\n",
      "\n",
      "Topic: 22 Word: 0.054*\"yeon\" + 0.040*\"chandu\" + 0.028*\"seoul\" + 0.018*\"swami\" + 0.016*\"hindu\" + 0.015*\"ratan\" + 0.013*\"katya\" + 0.013*\"delhi\" + 0.011*\"clan\" + 0.010*\"madra\"\n",
      "\n",
      "Topic: 23 Word: 0.042*\"ninja\" + 0.016*\"oleg\" + 0.016*\"malli\" + 0.014*\"malaysia\" + 0.012*\"feng\" + 0.011*\"chloe\" + 0.010*\"jessi\" + 0.009*\"ayesha\" + 0.008*\"marlow\" + 0.007*\"vinni\"\n",
      "\n",
      "Topic: 24 Word: 0.018*\"holm\" + 0.011*\"nadia\" + 0.010*\"android\" + 0.009*\"nora\" + 0.008*\"watson\" + 0.008*\"ching\" + 0.008*\"karim\" + 0.008*\"vanessa\" + 0.007*\"nikita\" + 0.007*\"erika\"\n",
      "\n",
      "Topic: 25 Word: 0.024*\"mala\" + 0.016*\"tanaka\" + 0.012*\"madelein\" + 0.009*\"rupert\" + 0.008*\"casey\" + 0.008*\"lora\" + 0.007*\"paulin\" + 0.007*\"airship\" + 0.007*\"simi\" + 0.007*\"barney\"\n",
      "\n",
      "Topic: 26 Word: 0.082*\"godzilla\" + 0.015*\"takashi\" + 0.014*\"jami\" + 0.013*\"bug\" + 0.012*\"valli\" + 0.011*\"beth\" + 0.011*\"malik\" + 0.006*\"lizzi\" + 0.006*\"jsdf\" + 0.006*\"tokyo\"\n",
      "\n",
      "Topic: 27 Word: 0.019*\"korean\" + 0.018*\"yong\" + 0.016*\"chun\" + 0.011*\"yogi\" + 0.011*\"pakistan\" + 0.011*\"samantha\" + 0.010*\"titan\" + 0.009*\"zack\" + 0.007*\"wang\" + 0.007*\"artagnan\"\n",
      "\n",
      "Topic: 28 Word: 0.037*\"suraj\" + 0.025*\"reiko\" + 0.015*\"clone\" + 0.015*\"marina\" + 0.012*\"benji\" + 0.010*\"reggi\" + 0.009*\"dino\" + 0.007*\"blacki\" + 0.007*\"muni\" + 0.006*\"elliot\"\n",
      "\n",
      "Topic: 29 Word: 0.022*\"doug\" + 0.020*\"vicki\" + 0.020*\"tang\" + 0.014*\"abbi\" + 0.013*\"asano\" + 0.012*\"shocker\" + 0.011*\"bike\" + 0.010*\"yusuf\" + 0.008*\"adel\" + 0.008*\"tess\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perforamance evaluation: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.37653598189353943\t Topic: 0.006*\"leav\" + 0.006*\"tell\" + 0.005*\"jack\" + 0.005*\"hous\" + 0.004*\"polic\"\n",
      "Score: 0.2920134365558624\t Topic: 0.014*\"love\" + 0.011*\"father\" + 0.010*\"marri\" + 0.009*\"famili\" + 0.008*\"friend\"\n",
      "Score: 0.27077716588974\t Topic: 0.009*\"tell\" + 0.009*\"leav\" + 0.006*\"go\" + 0.006*\"friend\" + 0.005*\"time\"\n",
      "Score: 0.05402151867747307\t Topic: 0.007*\"murder\" + 0.006*\"kill\" + 0.006*\"leav\" + 0.005*\"mari\" + 0.005*\"jimmi\"\n"
     ]
    }
   ],
   "source": [
    "# testing on unseen data... jersey shore\n",
    "import os\n",
    "f = open('shore.txt', 'r')\n",
    "# f = open('dream.txt', 'r')\n",
    "# f = open('endgame.txt', 'r')\n",
    "# f = open('cars.txt', 'r')\n",
    "content = f.read()\n",
    "bow_vector = dictionary.doc2bow(preprocess(content))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
