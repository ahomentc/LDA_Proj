{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data\n",
    "data = pd.read_csv('wiki_movie_plots_deduped.csv')\n",
    "data = np.array(data)\n",
    "plots = list(data[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/andrei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing data\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = list(map(preprocess, plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bartender is working at a saloon, serving drinks to customers. After he fills a stereotypically Irish man's bucket with beer, Carrie Nation and her followers burst inside. They assault the Irish man, pulling his hat over his eyes and then dumping the beer over his head. The group then begin wrecking the bar, smashing the fixtures, mirrors, and breaking the cash register. The bartender then sprays seltzer water in Nation's face before a group of policemen appear and order everybody to leave.[1]\n",
      "['bartend', 'work', 'saloon', 'serv', 'drink', 'custom', 'fill', 'stereotyp', 'irish', 'bucket', 'beer', 'carri', 'nation', 'follow', 'burst', 'insid', 'assault', 'irish', 'pull', 'eye', 'dump', 'beer', 'head', 'group', 'begin', 'wreck', 'smash', 'fixtur', 'mirror', 'break', 'cash', 'regist', 'bartend', 'spray', 'seltzer', 'water', 'nation', 'face', 'group', 'policemen', 'appear', 'order', 'everybodi', 'leav']\n"
     ]
    }
   ],
   "source": [
    "print(plots[0])\n",
    "print(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of the words\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "'''\n",
    "Filter out tokens that appear in less than 15 documents (absolute number) or\n",
    "more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary reporting how many words and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tfidf\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our lda model using bag of words\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=30, id2word=dictionary, passes=5, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"kill\" + 0.007*\"krishna\" + 0.006*\"tell\" + 0.006*\"famili\" + 0.005*\"leav\" + 0.005*\"devi\" + 0.005*\"return\" + 0.005*\"friend\" + 0.005*\"john\" + 0.005*\"go\"\n",
      "Topic: 1 \n",
      "Words: 0.012*\"kill\" + 0.006*\"attack\" + 0.005*\"escap\" + 0.004*\"forc\" + 0.004*\"destroy\" + 0.004*\"leav\" + 0.004*\"reveal\" + 0.004*\"ship\" + 0.004*\"earth\" + 0.004*\"return\"\n",
      "Topic: 2 \n",
      "Words: 0.009*\"tell\" + 0.009*\"leav\" + 0.006*\"go\" + 0.006*\"friend\" + 0.005*\"time\" + 0.005*\"life\" + 0.005*\"home\" + 0.005*\"come\" + 0.005*\"ask\" + 0.005*\"father\"\n",
      "Topic: 3 \n",
      "Words: 0.007*\"murder\" + 0.006*\"kill\" + 0.006*\"leav\" + 0.005*\"mari\" + 0.005*\"jimmi\" + 0.004*\"tell\" + 0.004*\"discov\" + 0.004*\"hous\" + 0.004*\"paul\" + 0.004*\"race\"\n",
      "Topic: 4 \n",
      "Words: 0.007*\"kill\" + 0.005*\"tell\" + 0.005*\"leav\" + 0.005*\"david\" + 0.004*\"polic\" + 0.004*\"take\" + 0.004*\"terrorist\" + 0.004*\"meet\" + 0.004*\"jam\" + 0.004*\"go\"\n",
      "Topic: 5 \n",
      "Words: 0.014*\"love\" + 0.011*\"father\" + 0.010*\"marri\" + 0.009*\"famili\" + 0.008*\"friend\" + 0.008*\"come\" + 0.007*\"mother\" + 0.006*\"get\" + 0.006*\"girl\" + 0.006*\"meet\"\n",
      "Topic: 6 \n",
      "Words: 0.007*\"radha\" + 0.006*\"leav\" + 0.005*\"love\" + 0.005*\"tell\" + 0.005*\"film\" + 0.005*\"harri\" + 0.004*\"take\" + 0.004*\"johnni\" + 0.004*\"marri\" + 0.004*\"wife\"\n",
      "Topic: 7 \n",
      "Words: 0.006*\"leav\" + 0.006*\"tell\" + 0.005*\"jack\" + 0.005*\"hous\" + 0.004*\"polic\" + 0.004*\"kill\" + 0.004*\"take\" + 0.004*\"go\" + 0.004*\"home\" + 0.004*\"return\"\n",
      "Topic: 8 \n",
      "Words: 0.010*\"kill\" + 0.007*\"king\" + 0.005*\"leav\" + 0.005*\"fight\" + 0.005*\"take\" + 0.005*\"power\" + 0.004*\"return\" + 0.004*\"tell\" + 0.004*\"help\" + 0.004*\"reveal\"\n",
      "Topic: 9 \n",
      "Words: 0.015*\"kill\" + 0.015*\"polic\" + 0.006*\"money\" + 0.006*\"gang\" + 0.006*\"offic\" + 0.006*\"take\" + 0.005*\"murder\" + 0.005*\"escap\" + 0.004*\"shoot\" + 0.004*\"plan\"\n"
     ]
    }
   ],
   "source": [
    "# explore the words occuring in each topic and their relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using TF-IDF\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=30, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.011*\"krishna\" + 0.007*\"devi\" + 0.007*\"chandu\" + 0.006*\"jong\" + 0.004*\"sook\" + 0.002*\"tang\" + 0.002*\"mala\" + 0.002*\"venu\" + 0.002*\"crux\" + 0.002*\"terrorist\"\n",
      "\n",
      "Topic: 1 Word: 0.003*\"film\" + 0.002*\"love\" + 0.002*\"jung\" + 0.002*\"famili\" + 0.002*\"father\" + 0.002*\"stori\" + 0.002*\"life\" + 0.002*\"mother\" + 0.002*\"villag\" + 0.002*\"girl\"\n",
      "\n",
      "Topic: 2 Word: 0.002*\"film\" + 0.001*\"love\" + 0.001*\"girl\" + 0.001*\"play\" + 0.001*\"school\" + 0.001*\"young\" + 0.001*\"year\" + 0.001*\"life\" + 0.001*\"student\" + 0.001*\"friend\"\n",
      "\n",
      "Topic: 3 Word: 0.007*\"raja\" + 0.003*\"singh\" + 0.002*\"rama\" + 0.002*\"kumar\" + 0.002*\"famili\" + 0.002*\"villag\" + 0.002*\"love\" + 0.002*\"father\" + 0.002*\"shyam\" + 0.002*\"kill\"\n",
      "\n",
      "Topic: 4 Word: 0.006*\"dong\" + 0.003*\"seoul\" + 0.003*\"vishnu\" + 0.003*\"yong\" + 0.002*\"korean\" + 0.001*\"korea\" + 0.001*\"kwon\" + 0.001*\"prison\" + 0.001*\"luke\" + 0.001*\"german\"\n",
      "\n",
      "Topic: 5 Word: 0.002*\"yuki\" + 0.002*\"king\" + 0.002*\"team\" + 0.002*\"villag\" + 0.002*\"school\" + 0.002*\"film\" + 0.002*\"famili\" + 0.002*\"love\" + 0.002*\"stori\" + 0.002*\"father\"\n",
      "\n",
      "Topic: 6 Word: 0.005*\"yakuza\" + 0.003*\"ninja\" + 0.003*\"japanes\" + 0.002*\"rana\" + 0.002*\"ship\" + 0.002*\"chun\" + 0.002*\"kill\" + 0.002*\"soldier\" + 0.002*\"reiko\" + 0.002*\"german\"\n",
      "\n",
      "Topic: 7 Word: 0.003*\"conan\" + 0.002*\"school\" + 0.002*\"guru\" + 0.002*\"love\" + 0.002*\"suraj\" + 0.002*\"girl\" + 0.002*\"polic\" + 0.002*\"film\" + 0.002*\"student\" + 0.002*\"father\"\n",
      "\n",
      "Topic: 8 Word: 0.003*\"yeon\" + 0.002*\"love\" + 0.002*\"priya\" + 0.002*\"marri\" + 0.002*\"father\" + 0.002*\"famili\" + 0.002*\"friend\" + 0.002*\"girl\" + 0.002*\"mother\" + 0.002*\"life\"\n",
      "\n",
      "Topic: 9 Word: 0.003*\"villag\" + 0.002*\"kill\" + 0.002*\"father\" + 0.002*\"famili\" + 0.002*\"girl\" + 0.002*\"stori\" + 0.002*\"mother\" + 0.002*\"love\" + 0.002*\"king\" + 0.002*\"polic\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perforamance evaluation: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.471373975276947\t Topic: 0.007*\"murder\" + 0.006*\"kill\" + 0.006*\"leav\" + 0.005*\"mari\" + 0.005*\"jimmi\"\n",
      "Score: 0.40686270594596863\t Topic: 0.006*\"leav\" + 0.006*\"tell\" + 0.005*\"jack\" + 0.005*\"hous\" + 0.004*\"polic\"\n",
      "Score: 0.06820639967918396\t Topic: 0.010*\"kill\" + 0.007*\"king\" + 0.005*\"leav\" + 0.005*\"fight\" + 0.005*\"take\"\n",
      "Score: 0.05128684267401695\t Topic: 0.012*\"kill\" + 0.006*\"attack\" + 0.005*\"escap\" + 0.004*\"forc\" + 0.004*\"destroy\"\n"
     ]
    }
   ],
   "source": [
    "# testing on unseen data... jersey shore\n",
    "import os\n",
    "# f = open('shore.txt', 'r')\n",
    "# f = open('dream.txt', 'r')\n",
    "# f = open('endgame.txt', 'r')\n",
    "f = open('cars.txt', 'r')\n",
    "content = f.read()\n",
    "bow_vector = dictionary.doc2bow(preprocess(content))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
