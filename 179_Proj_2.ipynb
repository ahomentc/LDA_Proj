{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/andrei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data\n",
    "# data = pd.read_csv('wiki_movie_plots_deduped.csv')\n",
    "data = pd.read_csv('inaug_speeches.csv', encoding='windows-1252')\n",
    "data = np.array(data)\n",
    "# plots = list(data[:,7])\n",
    "plots = list(data[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array with the lemmatized and stemmed data\n",
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    wordsArr = []\n",
    "    process1 = gensim.utils.simple_preprocess(text)\n",
    "    for token in process1:\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "#             wordsArr.append(stemmer.stem(lemmatizer.lemmatize(token)))\n",
    "#             wordsArr.append(lemmatizer.lemmatize(token))\n",
    "            wordsArr.append(token)\n",
    "    return wordsArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = list(map(preprocess, plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(clean_data))\n",
    "# print(plots[0])\n",
    "# print(clean_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of the words\n",
    "word_dict = gensim.corpora.Dictionary(clean_data)\n",
    "\n",
    "# filter out words that are in less than 20 documents and that are in above 20% of documents\n",
    "word_dict.filter_extremes(no_below=4, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter of how many times words appear\n",
    "word_counts = [word_dict.doc2bow(doc) for doc in clean_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tfidf\n",
    "tfidf = models.TfidfModel(word_counts)\n",
    "words_tfidf = tfidf[word_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using TF-IDF\n",
    "lda_model = gensim.models.LdaMulticore(words_tfidf, num_topics=20, id2word=word_dict, passes=5, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.002*\"specie\" + 0.002*\"dream\" + 0.002*\"necessarily\" + 0.002*\"deal\" + 0.002*\"urge\" + 0.001*\"treatment\" + 0.001*\"consummation\" + 0.001*\"desirable\" + 0.001*\"intentions\" + 0.001*\"administrations\" \n",
      "\n",
      "Topic: 1 Word: 0.004*\"importance\" + 0.004*\"circumstances\" + 0.003*\"sentiments\" + 0.003*\"principle\" + 0.003*\"effect\" + 0.003*\"objects\" + 0.003*\"event\" + 0.003*\"extent\" + 0.003*\"actual\" + 0.003*\"connected\" \n",
      "\n",
      "Topic: 2 Word: 0.004*\"story\" + 0.002*\"compassion\" + 0.001*\"commitment\" + 0.001*\"schools\" + 0.001*\"birth\" + 0.001*\"commitments\" + 0.001*\"jefferson\" + 0.001*\"ambitions\" + 0.001*\"petty\" + 0.001*\"build\" \n",
      "\n",
      "Topic: 3 Word: 0.003*\"jobs\" + 0.003*\"slaves\" + 0.002*\"cease\" + 0.002*\"workers\" + 0.002*\"survive\" + 0.002*\"righteous\" + 0.002*\"expressly\" + 0.002*\"city\" + 0.002*\"plainly\" + 0.002*\"address\" \n",
      "\n",
      "Topic: 4 Word: 0.004*\"today\" + 0.004*\"congress\" + 0.003*\"americans\" + 0.003*\"business\" + 0.003*\"economic\" + 0.003*\"peoples\" + 0.003*\"change\" + 0.003*\"question\" + 0.003*\"republic\" + 0.003*\"millions\" \n",
      "\n",
      "Topic: 5 Word: 0.004*\"dreams\" + 0.003*\"nuclear\" + 0.003*\"th\" + 0.003*\"century\" + 0.003*\"senator\" + 0.003*\"yes\" + 0.002*\"journey\" + 0.002*\"heroes\" + 0.002*\"dream\" + 0.002*\"goals\" \n",
      "\n",
      "Topic: 6 Word: 0.003*\"learned\" + 0.002*\"agitation\" + 0.002*\"whilst\" + 0.002*\"test\" + 0.002*\"upward\" + 0.002*\"slavery\" + 0.002*\"jurisdiction\" + 0.002*\"religious\" + 0.002*\"manner\" + 0.002*\"gain\" \n",
      "\n",
      "Topic: 7 Word: 0.002*\"improvements\" + 0.002*\"avail\" + 0.002*\"authorities\" + 0.002*\"errors\" + 0.001*\"diffusion\" + 0.001*\"councils\" + 0.001*\"repair\" + 0.001*\"awful\" + 0.001*\"resulting\" + 0.001*\"services\" \n",
      "\n",
      "Topic: 8 Word: 0.003*\"proposition\" + 0.002*\"extension\" + 0.001*\"acquisition\" + 0.001*\"treatment\" + 0.001*\"indian\" + 0.001*\"effects\" + 0.001*\"civilized\" + 0.001*\"scarcely\" + 0.001*\"services\" + 0.001*\"rapid\" \n",
      "\n",
      "Topic: 9 Word: 0.004*\"democracy\" + 0.004*\"mr\" + 0.004*\"friends\" + 0.004*\"thank\" + 0.003*\"help\" + 0.003*\"words\" + 0.003*\"women\" + 0.003*\"live\" + 0.003*\"moral\" + 0.003*\"bless\" \n",
      "\n",
      "Topic: 10 Word: 0.004*\"problems\" + 0.003*\"self\" + 0.003*\"generation\" + 0.003*\"cause\" + 0.003*\"money\" + 0.003*\"citizen\" + 0.003*\"promise\" + 0.003*\"act\" + 0.003*\"covenant\" + 0.003*\"courage\" \n",
      "\n",
      "Topic: 11 Word: 0.001*\"enforcement\" + 0.001*\"regulation\" + 0.001*\"controversies\" + 0.001*\"investigation\" + 0.001*\"federal\" + 0.001*\"rigid\" + 0.001*\"building\" + 0.000*\"advancement\" + 0.000*\"contributed\" + 0.000*\"enforce\" \n",
      "\n",
      "Topic: 12 Word: 0.002*\"generally\" + 0.001*\"defending\" + 0.001*\"militia\" + 0.001*\"worth\" + 0.001*\"accountability\" + 0.001*\"enjoins\" + 0.001*\"thanks\" + 0.001*\"progressive\" + 0.001*\"correction\" + 0.001*\"gallant\" \n",
      "\n",
      "Topic: 13 Word: 0.002*\"drawn\" + 0.002*\"creed\" + 0.002*\"journey\" + 0.002*\"evident\" + 0.001*\"requires\" + 0.001*\"founding\" + 0.001*\"lift\" + 0.001*\"liberate\" + 0.001*\"faction\" + 0.001*\"succeed\" \n",
      "\n",
      "Topic: 14 Word: 0.003*\"reposed\" + 0.003*\"thereof\" + 0.003*\"entertain\" + 0.002*\"magistrate\" + 0.002*\"ceremony\" + 0.002*\"punishment\" + 0.002*\"instance\" + 0.002*\"execution\" + 0.002*\"official\" + 0.002*\"requires\" \n",
      "\n",
      "Topic: 15 Word: 0.000*\"instituted\" + 0.000*\"candid\" + 0.000*\"internal\" + 0.000*\"sea\" + 0.000*\"theory\" + 0.000*\"construction\" + 0.000*\"doubts\" + 0.000*\"subordinate\" + 0.000*\"preparing\" + 0.000*\"formation\" \n",
      "\n",
      "Topic: 16 Word: 0.000*\"lesson\" + 0.000*\"manage\" + 0.000*\"disturbed\" + 0.000*\"enlarge\" + 0.000*\"flag\" + 0.000*\"flow\" + 0.000*\"forbearance\" + 0.000*\"humane\" + 0.000*\"lend\" + 0.000*\"delicate\" \n",
      "\n",
      "Topic: 17 Word: 0.002*\"partisan\" + 0.002*\"extravagance\" + 0.002*\"vicious\" + 0.002*\"usefulness\" + 0.002*\"governmental\" + 0.002*\"activity\" + 0.001*\"concession\" + 0.001*\"exact\" + 0.001*\"enforced\" + 0.001*\"thrift\" \n",
      "\n",
      "Topic: 18 Word: 0.004*\"revenue\" + 0.003*\"federal\" + 0.003*\"respect\" + 0.003*\"general\" + 0.003*\"earth\" + 0.003*\"institutions\" + 0.003*\"territory\" + 0.003*\"subject\" + 0.003*\"condition\" + 0.003*\"opinion\" \n",
      "\n",
      "Topic: 19 Word: 0.003*\"role\" + 0.002*\"policies\" + 0.001*\"structure\" + 0.001*\"retreat\" + 0.001*\"meeting\" + 0.001*\"record\" + 0.001*\"abroad\" + 0.001*\"leads\" + 0.001*\"lived\" + 0.001*\"boldly\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the generated topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {} \\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.33216527104377747\t Topic: 0.004*\"democracy\" + 0.004*\"mr\" + 0.004*\"friends\"\n",
      "Score: 0.12886330485343933\t Topic: 0.004*\"problems\" + 0.003*\"self\" + 0.003*\"generation\"\n",
      "Score: 0.12593846023082733\t Topic: 0.002*\"drawn\" + 0.002*\"creed\" + 0.002*\"journey\"\n",
      "Score: 0.11245398223400116\t Topic: 0.004*\"revenue\" + 0.003*\"federal\" + 0.003*\"respect\"\n",
      "Score: 0.11058087646961212\t Topic: 0.004*\"today\" + 0.004*\"congress\" + 0.003*\"americans\"\n",
      "Score: 0.10993235558271408\t Topic: 0.004*\"dreams\" + 0.003*\"nuclear\" + 0.003*\"th\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# f = open('shore.txt', 'r')\n",
    "# content = f.read()\n",
    "# content = 'We should, at all costs, reduce our carbon footprint and go green. We need solar panels.'\n",
    "# content = 'The battle began in the skies above us. In those first tense midnight hours, 1,000 aircraft roared overhead, with 17,000 allied airborne troops preparing to leap into the dark just beyond these trees. Then came dawn. The enemy who had occupied these heights saw the largest naval armada in the history of the world.'\n",
    "# content = \"There's new energy to harness, new jobs to be created, new schools to build, and threats to meet, alliances to repair\"\n",
    "content = \"Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time [applause] to lift our nation from the quicksands of racial injustice to the solid rock of brotherhood. Now is the time (Yes) [applause] (Now) to make justice a reality for all of Godâ€™s children.\"\n",
    "bow_vector = word_dict.doc2bow(preprocess(content))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
